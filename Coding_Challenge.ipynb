{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f7c575b-d7db-41da-9b99-76b4f0cddbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_datareader as pdr\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60833bc7-05e8-4bd0-9b2f-75affe0e0d85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def GetData(stocks, start_date,end_date):\n",
    "    #Assigns pandas dataframe and the average rfr as global variable\n",
    "    global E_rf, data\n",
    "    #Retrieves adjusted clsoe data for given securities and converts to log returns\n",
    "    data = pdr.DataReader(stocks, data_source='yahoo', start = start_date, end = end_date)['Adj Close']\n",
    "    data = (np.log(data) - np.log(data.shift(1))).dropna()\n",
    "    \n",
    "    #Retrieves daily yield for 5-year Tbill, takes the average and converts to a decimal percantage to match rest of data\n",
    "    E_rf = pdr.DataReader('^FVX', data_source='yahoo', start = start_date, end = end_date)['Adj Close']\n",
    "    E_rf = np.mean(E_rf)/100\n",
    "    \n",
    "    return\n",
    "\n",
    "GetData(['SPY','KO','TSLA'],'1-1-2021','1-1-2022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77de2534-87b7-4842-a5a1-7b9a94042370",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    First we'll test if the two seperate distributions of sample returns came from normal \n",
      "    distributions. This is done using the normaltest() function from the SciPy Stats library, \n",
      "    which implements a D’Agostino and Pearson’s test of normality. This test takes advantage\n",
      "    of the fact that individually, the test statistics for skewness and kurtosis are normally\n",
      "    distributed. Therefore, the sum of their squares has a chi-squared distribution with 2\n",
      "    degrees of freedom, which provides one option to test for normality.\n",
      "    \n",
      "KO p-value:   0.0001789\n",
      "TSLA p-value: 1e-07\n",
      "\n",
      "    With each test having p-values<.01, we can safely reject the null hypothesis that these \n",
      "    samples came from normally distributed populations at any reasonable confidence level.\n",
      "    While the D’Agostino and Pearson’s test only tests for normality with regards to skew and \n",
      "    kurtosis, the null hypothesis was rejected with high confidence so it's safe to say the \n",
      "    population distributions are not normaly distributed. The Levene test can now be used via \n",
      "    the levene() function from the SciPy Stats library to determine if there is a statistically \n",
      "    significant difference between population variances. \n",
      "    \n",
      "    A Levene test is more robust than a standard F-test, because it creates a test statistic\n",
      "    using transformed data rather than the raw data entered. This transformed data is the \n",
      "    absolute value of the deviation(difference) between each data point and the mean of the ith \n",
      "    subgroup. In our case the transformed data is the deviation between each data point and the \n",
      "    median of the ith subgroup making it a Brown–Forsythe test. From the previous test we know \n",
      "    our the population distributions are likely skewed, so using the Brown–Forsythe will help \n",
      "    compensate for this.\n",
      "    \n",
      "Levene p-value:   1.1995409840849466e-26\n",
      "\n",
      "    This extreme p-value rejects the null hypothesis that the variances of the two population \n",
      "    distributions are equal at any reasonable confidence level. With the formula for annualized\n",
      "    volatility being annualized standard deviation of returns, we can determine that because\n",
      "    the variances for KO and TSLA are statistically different, so are their volatilities. \n",
      "    \n",
      "\n",
      "    Now that we have determined that the difference in the varainces of the two samples is\n",
      "    statistically significant, we can test to see if there is a statisically significant \n",
      "    difference between the population means. This is done with the ttest_ind() function from the \n",
      "    SciPy Stats library, which is a students t-test that can be designated to assume unequal variances, \n",
      "    making it a Welch's t-test. \n",
      "    \n",
      "Welch's p-value:   0.690109\n",
      "\n",
      "    The large p-value determines that the null hypothesis is accepted, meaning the \n",
      "    two seprate samples come from populations with equal means. \n",
      "    \n",
      "    This is obviously counter-intuitive as the populations both are non-normal and have different \n",
      "    variances, so them sharing a mean doesn't make much sense. In the context of returns this outcome\n",
      "    starts to make more sense. A majority of distributions of stock returns could be described\n",
      "    as a skewed distribtuion with a mean about zero, with both the skew and mean being biased\n",
      "    towards the general trend of the stock. So it make sense that both KO and TSLA would share a mean\n",
      "    around zero, each scaled and skewed in regards to their overall performance. \n",
      "    \n",
      "    Due to both this result and the result of our D’Agostino and Pearson’s test, there may be concerns \n",
      "    regarding lack of normality. Normality is a condition to perform a t test, but with the two samples \n",
      "    being very large the test remains robust due to the CLT. To confirm we can look at the results from \n",
      "    a Wilcoxon test, another test between means which accounts for normality. The results for the Wilcoxon\n",
      "    test show the p-values for the Welch's and Wilcoxon test are similar, and confirms that we accept the \n",
      "    null hypothesis.\n",
      "    \n",
      "Wilcoxon p-value:   0.661576\n",
      "\n",
      "    The methods of testing used for this analysis, while thorough, are somewhat excessive considering\n",
      "    we are only dealing with two very large samples. That being said the tests conducted provided \n",
      "    some interesting insight into details of the population distributions, as well as generally being\n",
      "    more robust. \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "def statistics(x,y):\n",
    "    global KO_vol\n",
    "    \n",
    "    print('''\n",
    "    First we'll test if the two seperate distributions of sample returns came from normal \n",
    "    distributions. This is done using the normaltest() function from the SciPy Stats library, \n",
    "    which implements a D’Agostino and Pearson’s test of normality. This test takes advantage\n",
    "    of the fact that individually, the test statistics for skewness and kurtosis are normally\n",
    "    distributed. Therefore, the sum of their squares has a chi-squared distribution with 2\n",
    "    degrees of freedom, which provides one option to test for normality.\n",
    "    ''')\n",
    "    \n",
    "    k1,p1=sp.stats.normaltest(x)\n",
    "    k2,p2=sp.stats.normaltest(y)\n",
    "    \n",
    "    print(\"KO p-value:\",\" \",round(p1,7))\n",
    "    print(\"TSLA p-value:\",round(p2,7))\n",
    "    \n",
    "    print('''\n",
    "    With each test having p-values<.01, we can safely reject the null hypothesis that these \n",
    "    samples came from normally distributed populations at any reasonable confidence level.\n",
    "    While the D’Agostino and Pearson’s test only tests for normality with regards to skew and \n",
    "    kurtosis, the null hypothesis was rejected with high confidence so it's safe to say the \n",
    "    population distributions are not normaly distributed. The Levene test can now be used via \n",
    "    the levene() function from the SciPy Stats library to determine if there is a statistically \n",
    "    significant difference between population variances. \n",
    "    \n",
    "    A Levene test is more robust than a standard F-test, because it creates a test statistic\n",
    "    using transformed data rather than the raw data entered. This transformed data is the \n",
    "    absolute value of the deviation(difference) between each data point and the mean of the ith \n",
    "    subgroup. In our case the transformed data is the deviation between each data point and the \n",
    "    median of the ith subgroup making it a Brown–Forsythe test. From the previous test we know \n",
    "    our the population distributions are likely skewed, so using the Brown–Forsythe will help \n",
    "    compensate for this.\n",
    "    ''')\n",
    "    \n",
    "    l,L_p=sp.stats.levene(x,y, center = \"median\")\n",
    "    \n",
    "    print(\"Levene p-value:\",\" \",L_p)\n",
    "    \n",
    "    print('''\n",
    "    This extreme p-value rejects the null hypothesis that the variances of the two population \n",
    "    distributions are equal at any reasonable confidence level. With the formula for annualized\n",
    "    volatility being annualized standard deviation of returns, we can determine that because\n",
    "    the variances for KO and TSLA are statistically different, so are their volatilities. \n",
    "    ''')\n",
    "\n",
    "    print('''\n",
    "    Now that we have determined that the difference in the varainces of the two samples is\n",
    "    statistically significant, we can test to see if there is a statisically significant \n",
    "    difference between the population means. This is done with the ttest_ind() function from the \n",
    "    SciPy Stats library, which is a students t-test that can be designated to assume unequal variances, \n",
    "    making it a Welch's t-test. \n",
    "    ''')\n",
    "    \n",
    "    W,W_p=sp.stats.ttest_ind(x,y, equal_var = False )\n",
    "    \n",
    "    print(\"Welch's p-value:\",\" \",round(W_p,6))\n",
    "    \n",
    "    print('''\n",
    "    The large p-value determines that the null hypothesis is accepted, meaning the \n",
    "    two seprate samples come from populations with equal means. \n",
    "    \n",
    "    This is obviously counter-intuitive as the populations both are non-normal and have different \n",
    "    variances, so them sharing a mean doesn't make much sense. In the context of returns this outcome\n",
    "    starts to make more sense. A majority of distributions of stock returns could be described\n",
    "    as a skewed distribtuion with a mean about zero, with both the skew and mean being biased\n",
    "    towards the general trend of the stock. So it make sense that both KO and TSLA would share a mean\n",
    "    around zero, each scaled and skewed in regards to their overall performance. \n",
    "    \n",
    "    Due to both this result and the result of our D’Agostino and Pearson’s test, there may be concerns \n",
    "    regarding lack of normality. Normality is a condition to perform a t test, but with the two samples \n",
    "    being very large the test remains robust due to the CLT. To confirm we can look at the results from \n",
    "    a Wilcoxon test, another test between means which accounts for normality. The results for the Wilcoxon\n",
    "    test show the p-values for the Welch's and Wilcoxon test are similar, and confirms that we accept the \n",
    "    null hypothesis.\n",
    "    ''')\n",
    "\n",
    "    W2,W_p2=sp.stats.wilcoxon(x,y)\n",
    "    \n",
    "    print(\"Wilcoxon p-value:\",\" \",round(W_p2,6))\n",
    "    \n",
    "    print('''\n",
    "    The methods of testing used for this analysis, while thorough, are somewhat excessive considering\n",
    "    we are only dealing with two very large samples. That being said the tests conducted provided \n",
    "    some interesting insight into details of the population distributions, as well as generally being\n",
    "    more robust. \n",
    "    ''')\n",
    "\n",
    "    return\n",
    "    \n",
    "statistics(data[\"KO\"],data[\"TSLA\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7153da20-d86a-4bf7-a03e-b2b2524d738a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Annualized volatiliy is calculated using the simple formula of the standard deviation of the returns \n",
      "    multiplied by the square root of the number of trading days. \n",
      "    \n",
      "Volatility of KO: 14.75%\n",
      "\n",
      "    \n",
      "    \n",
      "    95% value at risk is calculated both historically and via simulation. Simulation isn't \n",
      "    the most robust method, it is parametric and since we determined that the population \n",
      "    distribution is non-normal without figuring out it's specific distribution, using parametric\n",
      "    methods for estimating value at risk is somewhat illogical. It's remains to show how the CLT\n",
      "    still allows the simulation to make a reasonable estimation when compared to the \n",
      "    historical(non-parametric) statistics. \n",
      "    \n",
      "Historical 95% Value at Risk: -24.41%\n",
      "Simulated 95% Value at Risk: -23.32%\n",
      "\n",
      "    This means that we can be 95% confident that over the next year, KO wont suffer losses worse\n",
      "    than -0.24414124450767702\n",
      "\n",
      "    \n",
      "    \n",
      "    Sharpe ratio is calulated with the standard formula of the mean of returns devided by the \n",
      "    standard deviation of returns. Included is both the Sharpe Ratio for returns and excess returns.\n",
      "    For the risk-free rate I'm using the average yield of the 5-year treasury bond throughout 2021. \n",
      "    \n",
      "Sharpe Ratio: 0.06277064432074195\n",
      "Sharpe Ratio(Excess Returns): -0.860449564894466\n",
      "\n",
      "    With both Sharpe Ratios being less than one, neither provides an optimistic result for the future\n",
      "    returns of KO.\n",
      "    \n",
      "  \n",
      "    \n",
      "    Dowside deviation is calulated by first isolating all returns less than the minimum acceptable\n",
      "    return(MAR), which in our case is the risk-free rate discussed earlier. The root sum squared of\n",
      "    these isolated returns is calculated to yield the daily dowside deviation, which when multiplied\n",
      "    by the square root of 252 yields the annualized downside deviation. \n",
      "    \n",
      "Downside Deviation: 20.69%\n",
      "\n",
      "    \n",
      "    \n",
      "    Maximum drawdown is the trickiest metric to find in terms of data manipulation. First the daily\n",
      "    returns for KO are converted into daily cumulative returns. The function cummax() is then used, \n",
      "    which returns the maximum cumulative return prior to and including the given day. The ratio\n",
      "    of maximum cumulative returns over current cumulative returns yields the current drawdown(if there\n",
      "    is one). The maximum of these ratios is the maximum drawdown for the year 2021.\n",
      "    \n",
      "Maximum Drawdown: -8.85%\n"
     ]
    }
   ],
   "source": [
    "def metrics(x,rf):\n",
    "    \n",
    "    print('''\n",
    "    Annualized volatiliy is calculated using the simple formula of the standard deviation of the returns \n",
    "    multiplied by the square root of the number of trading days. \n",
    "    ''')\n",
    "    \n",
    "    KO_vol=np.std(x)*np.sqrt(252)\n",
    "    print('Volatility of KO:', str(round(KO_vol*100,2))+\"%\")\n",
    "    \n",
    "    print('''\n",
    "    \n",
    "    \n",
    "    95% value at risk is calculated both historically and via simulation. Simulation isn't \n",
    "    the most robust method, it is parametric and since we determined that the population \n",
    "    distribution is non-normal without figuring out it's specific distribution, using parametric\n",
    "    methods for estimating value at risk is somewhat illogical. It's remains to show how the CLT\n",
    "    still allows the simulation to make a reasonable estimation when compared to the \n",
    "    historical(non-parametric) statistics. \n",
    "    ''')\n",
    "    sim_returns = np.random.normal(np.mean(x), np.std(x), 1000000)\n",
    "    \n",
    "    H_VAR_95 = np.percentile(x, 5)*np.sqrt(252)\n",
    "    S_VAR_95 = np.percentile(sim_returns, 5)*np.sqrt(252)\n",
    "    \n",
    "    print('Historical 95% Value at Risk:', str(round(H_VAR_95*100,2))+\"%\")\n",
    "    print('Simulated 95% Value at Risk:', str(round(S_VAR_95*100,2))+\"%\")\n",
    "    \n",
    "    print('''\n",
    "    This means that we can be 95% confident that over the next year, KO wont suffer losses worse\n",
    "    than''', H_VAR_95)\n",
    "    \n",
    "    print('''\n",
    "    \n",
    "    \n",
    "    Sharpe ratio is calulated with the standard formula of the mean of returns devided by the \n",
    "    standard deviation of returns. Included is both the Sharpe Ratio for returns and excess returns.\n",
    "    For the risk-free rate I'm using the average yield of the 5-year treasury bond throughout 2021. \n",
    "    ''')\n",
    "    \n",
    "    sharpe_KO=np.mean(x)/(np.std(x))\n",
    "    sharpe_KO2=(np.mean(x)-rf)/np.std(x)\n",
    "    \n",
    "    print('Sharpe Ratio:', sharpe_KO)\n",
    "    print('Sharpe Ratio(Excess Returns):', sharpe_KO2)\n",
    "    \n",
    "    print('''\n",
    "    With both Sharpe Ratios being less than one, neither provides an optimistic result for the future\n",
    "    returns of KO.\n",
    "    ''')\n",
    "   \n",
    "    print('''  \n",
    "    \n",
    "    Dowside deviation is calulated by first isolating all returns less than the minimum acceptable\n",
    "    return(MAR), which in our case is the risk-free rate discussed earlier. The root sum squared of\n",
    "    these isolated returns is calculated to yield the daily dowside deviation, which when multiplied\n",
    "    by the square root of 252 yields the annualized downside deviation. \n",
    "    ''')\n",
    "\n",
    "    sub_MAR_ret=(x-rf)[(x-rf)<0]\n",
    "    Down_dev=np.sqrt(sum((sub_MAR_ret)**2)/(len(sub_MAR_ret)))*np.sqrt(252)\n",
    "\n",
    "    print('Downside Deviation:', str(round(Down_dev*100,2))+\"%\")\n",
    "    \n",
    "    print('''\n",
    "    \n",
    "    \n",
    "    Maximum drawdown is the trickiest metric to find in terms of data manipulation. First the daily\n",
    "    returns for KO are converted into daily cumulative returns. The function cummax() is then used, \n",
    "    which returns the maximum cumulative return prior to and including the given day. The ratio\n",
    "    of maximum cumulative returns over current cumulative returns yields the current drawdown(if there\n",
    "    is one). The maximum of these ratios is the maximum drawdown for the year 2021.\n",
    "    ''')\n",
    "    cum_ret = np.cumsum(x)\n",
    "    DD = (1 + cum_ret.cummax())/(1 + cum_ret) - 1\n",
    "    print('Maximum Drawdown:', (str(-round(max(DD)*100,2))+\"%\"))\n",
    "    \n",
    "    return\n",
    "\n",
    "metrics(data[\"KO\"],E_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a6c6a9c-3b13-4745-816a-f123e8a66f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    First the cumulative returns for 2021 are calculated for both SPY and TSLA, these cumulative returns\n",
      "    will serve as our \"benmark\" returns.  \n",
      "    \n",
      "    I calculated beta two methods which yielded similar results. The first is using a linear regression \n",
      "    model, where TSLA returns are modeled as a function of market(SPY) returns. The coefficient for SPY\n",
      "    would then be our beta for the CAPM model. The Second method was using the covariance/variance method,\n",
      "    where the covariance of returns is divided by the variance of SPY to get beta. The formulas for alpha\n",
      "    remain the same but use the two seperately calculated betas.\n",
      "    \n",
      "Beta(regression): 1.955\n",
      "Alpha(regression): -0.314\n",
      "\n",
      "Beta(cov): 1.948\n",
      "Alpha(cov): -0.312\n",
      "\n",
      "    The results from the two methods are very similar. Both calculations for beta show that TSLA is nearly\n",
      "    twice as volatile as the benchmark. This helps explain our low value for alpha. \n",
      "    \n",
      "    Alpha is the difference between TSLA returns for 2021 and the expected market returns assuming the \n",
      "    market shares the same volatility as TSLA. With alpha being negative, it shows that if the market\n",
      "    was as volatile as TSLA it would likely outperform it, so the volatility of TSLA isn't desirable for\n",
      "    investors.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "def CAPM(x,y,rf):\n",
    "    print('''\n",
    "    First the cumulative returns for 2021 are calculated for both SPY and TSLA, these cumulative returns\n",
    "    will serve as our \"benmark\" returns.  \n",
    "    \n",
    "    I calculated beta two methods which yielded similar results. The first is using a linear regression \n",
    "    model, where TSLA returns are modeled as a function of market(SPY) returns. The coefficient for SPY\n",
    "    would then be our beta for the CAPM model. The Second method was using the covariance/variance method,\n",
    "    where the covariance of returns is divided by the variance of SPY to get beta. The formulas for alpha\n",
    "    remain the same but use the two seperately calculated betas.\n",
    "    ''')\n",
    "    bench_mkt = np.prod(x+1)-1\n",
    "    bench_ret = np.prod(y+1)-1\n",
    "    \n",
    "    #Beta calculated via covariance/varaince method\n",
    "    beta2=np.cov(x,y)[1,0]/np.var(x)\n",
    "    #Beta calculated via linear regression\n",
    "    beta = sp.stats.linregress(x, y= y)[0]\n",
    "    #Alpha calculated via covariance/varaince method\n",
    "    alpha2 = bench_ret-(rf+(beta2*(bench_mkt-rf)))\n",
    "    #Alpha calculated via linear regression\n",
    "    alpha = bench_ret-(rf+(beta*(bench_mkt-rf)))\n",
    "    \n",
    "\n",
    "    print('Beta(regression):', round(beta2,3))\n",
    "    print('Alpha(regression):', round(alpha2,3))\n",
    "    print()\n",
    "    print('Beta(cov):', round(beta,3))\n",
    "    print('Alpha(cov):', round(alpha,3))\n",
    "    \n",
    "    print('''\n",
    "    The results from the two methods are very similar. Both calculations for beta show that TSLA is nearly\n",
    "    twice as volatile as the benchmark. This helps explain our low value for alpha. \n",
    "    \n",
    "    Alpha is the difference between TSLA returns for 2021 and the expected market returns assuming the \n",
    "    market shares the same volatility as TSLA. With alpha being negative, it shows that if the market\n",
    "    was as volatile as TSLA it would likely outperform it, so the volatility of TSLA isn't desirable for\n",
    "    investors.\n",
    "    ''')\n",
    "    return\n",
    "    \n",
    "CAPM(data[\"SPY\"],data[\"TSLA\"],E_rf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
